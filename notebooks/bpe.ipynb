{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae472bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs336-basics \u001b[36m1.0.6\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!uv version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30eaae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46f6ca",
   "metadata": {},
   "source": [
    "### Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1c9514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308a8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0) #this gives you __repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf2eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0)) # this gives you __str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49b9b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a685d8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925424a",
   "metadata": {},
   "source": [
    "### Unicode Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8e98068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "<class 'bytes'>\n",
      "13\n",
      "23\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(list(utf8_encoded))\n",
    "print(type(utf8_encoded))\n",
    "\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e4c37b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello'\n",
      "[104, 101, 108, 108, 111]\n",
      "<class 'bytes'>\n",
      "5\n",
      "5\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(list(utf8_encoded))\n",
    "print(type(utf8_encoded))\n",
    "\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "5\n",
      "24\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "\n",
    "print(len(test_string))\n",
    "print(len(utf32_encoded))\n",
    "print(utf8_encoded.decode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fda10f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello! こんにちは!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28mbytes\u001b[39m([b]).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello! こんにちは!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ec025",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab0af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74baf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "iterator = re.finditer(PAT,  \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29611b",
   "metadata": {},
   "source": [
    "### Compute BPE Merges (Thought Exp)\n",
    "\n",
    "1. parallelization for pretokenization: chunk the text corpus by special tokens and process in parallel\n",
    "2. remove special tokens\n",
    "3. pre-tokenization: subwords (via regex) freq dict\n",
    "4. find the byte pair freq (within subwords): (A,B) -> freq\n",
    "5. identify: the pair (A,B) with highest frequency\n",
    "6. (incremental) update\n",
    "7. back to step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fadf45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([i for i in range(0,256)])\n",
    "pretokenizers = defaultdict(int)\n",
    "example_text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest こんにちは こんにちは\"\n",
    "match_iter =  re.finditer(PAT, example_text)\n",
    "\n",
    "for match in match_iter:\n",
    "    subword = match.group()\n",
    "    utf8_encoded = subword.encode(\"utf-8\")\n",
    "    pretokenizers[tuple(utf8_encoded)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9bee2957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {(32, 108, 111, 119): 4,\n",
      "             (32, 108, 111, 119, 101, 114): 2,\n",
      "             (32, 110, 101, 119, 101, 115, 116): 6,\n",
      "             (32, 119, 105, 100, 101, 115, 116): 3,\n",
      "             (32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175): 2,\n",
      "             (108, 111, 119): 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(pretokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7f075332",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict = defaultdict(int)\n",
    "for pretoken, cnt in pretokenizers.items():\n",
    "    for idx in range(len(pretoken)-1):\n",
    "        freq_dict[(pretoken[idx], pretoken[idx+1])]+=cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c8252023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(108, 111): 7,\n",
       "             (111, 119): 7,\n",
       "             (32, 108): 6,\n",
       "             (119, 101): 8,\n",
       "             (101, 114): 2,\n",
       "             (32, 119): 3,\n",
       "             (119, 105): 3,\n",
       "             (105, 100): 3,\n",
       "             (100, 101): 3,\n",
       "             (101, 115): 9,\n",
       "             (115, 116): 9,\n",
       "             (32, 110): 6,\n",
       "             (110, 101): 6,\n",
       "             (101, 119): 6,\n",
       "             (32, 227): 2,\n",
       "             (227, 129): 8,\n",
       "             (129, 147): 2,\n",
       "             (147, 227): 4,\n",
       "             (227, 130): 2,\n",
       "             (130, 147): 2,\n",
       "             (129, 171): 2,\n",
       "             (171, 227): 2,\n",
       "             (129, 161): 2,\n",
       "             (161, 227): 2,\n",
       "             (129, 175): 2})"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "153519a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pretoken(t: tuple, pattern: tuple) -> tuple:\n",
    "    result = []\n",
    "    i = 0\n",
    "    n = len(pattern)\n",
    "    while i < len(t):\n",
    "        if t[i:i+n] == pattern:\n",
    "            result.append(pattern)\n",
    "            i += n\n",
    "        else:\n",
    "            result.append(t[i])\n",
    "            i += 1\n",
    "\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fb618656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 116)\n"
     ]
    }
   ],
   "source": [
    "new_token = max(freq_dict.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "print(new_token)\n",
    "vocab.add(new_token)\n",
    "del freq_dict[new_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9f8dd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update\n",
    "\n",
    "pretoken_changes = defaultdict(int)\n",
    "for pretoken, cnt in pretokenizers.items():\n",
    "    \n",
    "    changes = defaultdict(int)\n",
    "    reform_pretoken = False\n",
    "    for idx in range(len(pretoken)-1):\n",
    "        if (pretoken[idx], pretoken[idx+1]) == new_token:\n",
    "            reform_pretoken = True\n",
    "            if idx >= 1:\n",
    "                changes[(pretoken[idx-1], pretoken[idx])] -= cnt\n",
    "                changes[(pretoken[idx-1], new_token)] += cnt\n",
    "            if idx < len(pretoken)-2:\n",
    "                changes[(pretoken[idx+1], pretoken[idx+2])] -= cnt\n",
    "                changes[(new_token, pretoken[idx+2])] += cnt\n",
    "    if reform_pretoken:\n",
    "        for k,v in changes.items():\n",
    "            freq_dict[k] += v\n",
    "        new_pretoken = update_pretoken(pretoken, new_token)\n",
    "        pretoken_changes[(pretoken, new_pretoken)] = cnt\n",
    "        \n",
    "for change, cnt in pretoken_changes.items():\n",
    "    pt = change[0]\n",
    "    npt = change[1]\n",
    "    del pretokenizers[pt]\n",
    "    pretokenizers[npt] = cnt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cb3abf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {(32, 108, 111, 119): 4,\n",
      "             (32, 108, 111, 119, 101, 114): 2,\n",
      "             (32, 110, 101, 119, 101, (115, 116)): 6,\n",
      "             (32, 119, 105, 100, 101, (115, 116)): 3,\n",
      "             (32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175): 2,\n",
      "             (108, 111, 119): 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(pretokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "806d2b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {(32, 108): 6,\n",
      "             (32, 110): 6,\n",
      "             (32, 119): 3,\n",
      "             (32, 227): 2,\n",
      "             (100, 101): 3,\n",
      "             (101, 114): 2,\n",
      "             (101, (115, 116)): 9,\n",
      "             (101, 115): 0,\n",
      "             (101, 119): 6,\n",
      "             (105, 100): 3,\n",
      "             (108, 111): 7,\n",
      "             (110, 101): 6,\n",
      "             (111, 119): 7,\n",
      "             (119, 101): 8,\n",
      "             (119, 105): 3,\n",
      "             (129, 147): 2,\n",
      "             (129, 161): 2,\n",
      "             (129, 171): 2,\n",
      "             (129, 175): 2,\n",
      "             (130, 147): 2,\n",
      "             (147, 227): 4,\n",
      "             (161, 227): 2,\n",
      "             (171, 227): 2,\n",
      "             (227, 129): 8,\n",
      "             (227, 130): 2})\n"
     ]
    }
   ],
   "source": [
    "pprint(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a647d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0,\n",
      " 1,\n",
      " 2,\n",
      " 3,\n",
      " 4,\n",
      " 5,\n",
      " 6,\n",
      " 7,\n",
      " 8,\n",
      " 9,\n",
      " 10,\n",
      " 11,\n",
      " 12,\n",
      " 13,\n",
      " 14,\n",
      " 15,\n",
      " 16,\n",
      " 17,\n",
      " 18,\n",
      " 19,\n",
      " 20,\n",
      " 21,\n",
      " 22,\n",
      " 23,\n",
      " 24,\n",
      " 25,\n",
      " 26,\n",
      " 27,\n",
      " 28,\n",
      " 29,\n",
      " 30,\n",
      " 31,\n",
      " 32,\n",
      " 33,\n",
      " 34,\n",
      " 35,\n",
      " 36,\n",
      " 37,\n",
      " 38,\n",
      " 39,\n",
      " 40,\n",
      " 41,\n",
      " 42,\n",
      " 43,\n",
      " 44,\n",
      " 45,\n",
      " 46,\n",
      " 47,\n",
      " 48,\n",
      " 49,\n",
      " 50,\n",
      " 51,\n",
      " 52,\n",
      " 53,\n",
      " 54,\n",
      " 55,\n",
      " 56,\n",
      " 57,\n",
      " 58,\n",
      " 59,\n",
      " 60,\n",
      " 61,\n",
      " 62,\n",
      " 63,\n",
      " 64,\n",
      " 65,\n",
      " 66,\n",
      " 67,\n",
      " 68,\n",
      " 69,\n",
      " 70,\n",
      " 71,\n",
      " 72,\n",
      " 73,\n",
      " 74,\n",
      " 75,\n",
      " 76,\n",
      " 77,\n",
      " 78,\n",
      " 79,\n",
      " 80,\n",
      " 81,\n",
      " 82,\n",
      " 83,\n",
      " 84,\n",
      " 85,\n",
      " 86,\n",
      " 87,\n",
      " 88,\n",
      " 89,\n",
      " 90,\n",
      " 91,\n",
      " 92,\n",
      " 93,\n",
      " 94,\n",
      " 95,\n",
      " 96,\n",
      " 97,\n",
      " 98,\n",
      " 99,\n",
      " 100,\n",
      " 101,\n",
      " 102,\n",
      " 103,\n",
      " 104,\n",
      " 105,\n",
      " 106,\n",
      " 107,\n",
      " 108,\n",
      " 109,\n",
      " 110,\n",
      " 111,\n",
      " 112,\n",
      " 113,\n",
      " 114,\n",
      " 115,\n",
      " 116,\n",
      " 117,\n",
      " 118,\n",
      " 119,\n",
      " 120,\n",
      " 121,\n",
      " 122,\n",
      " 123,\n",
      " 124,\n",
      " 125,\n",
      " 126,\n",
      " 127,\n",
      " 128,\n",
      " 129,\n",
      " 130,\n",
      " 131,\n",
      " 132,\n",
      " 133,\n",
      " 134,\n",
      " 135,\n",
      " 136,\n",
      " 137,\n",
      " 138,\n",
      " 139,\n",
      " 140,\n",
      " 141,\n",
      " 142,\n",
      " 143,\n",
      " 144,\n",
      " 145,\n",
      " 146,\n",
      " 147,\n",
      " 148,\n",
      " 149,\n",
      " 150,\n",
      " 151,\n",
      " 152,\n",
      " 153,\n",
      " 154,\n",
      " 155,\n",
      " 156,\n",
      " 157,\n",
      " 158,\n",
      " 159,\n",
      " 160,\n",
      " 161,\n",
      " 162,\n",
      " 163,\n",
      " 164,\n",
      " 165,\n",
      " 166,\n",
      " 167,\n",
      " 168,\n",
      " 169,\n",
      " 170,\n",
      " 171,\n",
      " 172,\n",
      " 173,\n",
      " 174,\n",
      " 175,\n",
      " 176,\n",
      " 177,\n",
      " 178,\n",
      " 179,\n",
      " 180,\n",
      " 181,\n",
      " 182,\n",
      " 183,\n",
      " 184,\n",
      " 185,\n",
      " 186,\n",
      " 187,\n",
      " 188,\n",
      " 189,\n",
      " 190,\n",
      " 191,\n",
      " 192,\n",
      " 193,\n",
      " 194,\n",
      " 195,\n",
      " 196,\n",
      " 197,\n",
      " 198,\n",
      " 199,\n",
      " 200,\n",
      " 201,\n",
      " 202,\n",
      " 203,\n",
      " 204,\n",
      " 205,\n",
      " 206,\n",
      " 207,\n",
      " 208,\n",
      " 209,\n",
      " 210,\n",
      " 211,\n",
      " 212,\n",
      " 213,\n",
      " 214,\n",
      " 215,\n",
      " 216,\n",
      " 217,\n",
      " 218,\n",
      " 219,\n",
      " 220,\n",
      " 221,\n",
      " 222,\n",
      " 223,\n",
      " 224,\n",
      " 225,\n",
      " 226,\n",
      " 227,\n",
      " 228,\n",
      " 229,\n",
      " 230,\n",
      " 231,\n",
      " 232,\n",
      " 233,\n",
      " 234,\n",
      " 235,\n",
      " 236,\n",
      " 237,\n",
      " 238,\n",
      " 239,\n",
      " 240,\n",
      " 241,\n",
      " 242,\n",
      " 243,\n",
      " 244,\n",
      " 245,\n",
      " 246,\n",
      " 247,\n",
      " 248,\n",
      " 249,\n",
      " 250,\n",
      " 251,\n",
      " 252,\n",
      " 253,\n",
      " 254,\n",
      " 255,\n",
      " (115, 116)}\n"
     ]
    }
   ],
   "source": [
    "pprint(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91455e0",
   "metadata": {},
   "source": [
    "### Complete BPE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3c6e51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from pprint import pprint\n",
    "\n",
    "def train_bpe(text: str, target_vocab_size: int, debug = False):\n",
    "    \"\"\"\n",
    "    Trains a byte-pair encoding (BPE) tokenizer on `text` until `target_vocab_size` is reached.\n",
    "\n",
    "    Returns:\n",
    "        vocab (dict[int, bytes]): Final vocabulary mapping token IDs to byte tokens.\n",
    "        merges (list[tuple[bytes, bytes]]): List of BPE merges in creation order.\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    vocab = [bytes([i]) for i in range(256)]\n",
    "\n",
    "    def merge_pretoken(tokens: tuple, pair: tuple) -> tuple:\n",
    "        merged = b''.join(pair)\n",
    "        result, i, n = [], 0, len(pair)\n",
    "        while i < len(tokens):\n",
    "            if tokens[i:i+n] == pair:\n",
    "                result.append(merged)\n",
    "                i += n\n",
    "            else:\n",
    "                result.append(tokens[i])\n",
    "                i += 1\n",
    "        return tuple(result)\n",
    "\n",
    "    # initial pretokenization\n",
    "    token_counts = defaultdict(int)\n",
    "    for m in re.finditer(pattern, text):\n",
    "        b = m.group().encode(\"utf-8\")\n",
    "        token_counts[tuple(bytes([x]) for x in b)] += 1\n",
    "\n",
    "    if debug:\n",
    "        print(\"Initial pretokens:\")\n",
    "        pprint(token_counts)\n",
    "\n",
    "    # initial pair frequencies\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for token, count in token_counts.items():\n",
    "        for i in range(len(token) - 1):\n",
    "            pair_freqs[(token[i], token[i + 1])] += count\n",
    "    if debug:\n",
    "        print(\"\\nInitial pair frequencies:\")\n",
    "        pprint(pair_freqs)\n",
    "\n",
    "    merges = []\n",
    "    while len(vocab) < target_vocab_size:\n",
    "        if not pair_freqs:\n",
    "            print(\"No more pairs to merge\")\n",
    "            break\n",
    "\n",
    "        new_pair = max(pair_freqs.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "        left, right = new_pair\n",
    "        merged_token = left + right\n",
    "        if debug:\n",
    "            print(f\"\\nMerging: {new_pair} -> {merged_token}\")\n",
    "        merges.append(new_pair)\n",
    "        vocab.append(merged_token)\n",
    "        del pair_freqs[new_pair]\n",
    "\n",
    "        pretoken_updates = {}\n",
    "        for token, count in token_counts.items():\n",
    "            local_changes = defaultdict(int)\n",
    "            has_merge = False\n",
    "\n",
    "            for i in range(len(token) - 1):\n",
    "                if (token[i], token[i + 1]) == new_pair:\n",
    "                    has_merge = True\n",
    "                    if i >= 1:\n",
    "                        local_changes[(token[i - 1], token[i])] -= count\n",
    "                        local_changes[(token[i - 1], merged_token)] += count\n",
    "                    if i < len(token) - 2:\n",
    "                        local_changes[(token[i + 1], token[i + 2])] -= count\n",
    "                        local_changes[(merged_token, token[i + 2])] += count\n",
    "\n",
    "            if has_merge:\n",
    "                for k, v in local_changes.items():\n",
    "                    pair_freqs[k] += v\n",
    "                    if pair_freqs[k] <= 0:\n",
    "                        del pair_freqs[k]\n",
    "                new_token = merge_pretoken(token, (left, right))\n",
    "                pretoken_updates[token] = (new_token, count)\n",
    "\n",
    "        for old, (new, c) in pretoken_updates.items():\n",
    "            del token_counts[old]\n",
    "            token_counts[new] = c\n",
    "        if debug:\n",
    "            print(token_counts)\n",
    "\n",
    "    vocab = {i: token for i, token in enumerate(vocab)}\n",
    "    return vocab, merges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "388f7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\n",
    "vocab, merges = train_bpe(example_text, 256+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b2d258ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'st',\n",
       " 257: b'est',\n",
       " 258: b'ow',\n",
       " 259: b'low',\n",
       " 260: b'west',\n",
       " 261: b'ne'}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "edf03c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e')]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0fd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
